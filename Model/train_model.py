# -*- coding: utf-8 -*-
"""(raytune) weak-ensembles-bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exrQZQ4uXk0XREHsVftir1fKC9iLx_V4
"""

# !pip install -U optuna ray[tune]

import math
import os
import random
import time
from pathlib import Path

import numpy as np
import pandas as pd
import ray
import torch
from sklearn.metrics import f1_score

# os.environ['HF_ENDPOINT'] = 'hf-cdn.sufy.com'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

if torch.cuda.is_available():
    gpu_count = torch.cuda.device_count()
else:
    gpu_count = 0
print(f"{gpu_count} GPUs are available.")
num_cpus = os.cpu_count()
ray.init(num_cpus=num_cpus, num_gpus=gpu_count, ignore_reinit_error=True)

path = "/mnt/hdd/zm/code/temp/"

os.chdir(path)
os.listdir(path)


# !nvidia-smi

# helper method
def l2_norm(weights):
    return np.linalg.norm(weights)


def sigmoid(x):
    return 1.0 / (1.0 + math.exp(-x))


# 0. the metrifcs used in model training and prediction evaluation

# cross entropy loss, soft evaluation
def cross_entropy(targets, predictions, epsilon=1e-12):
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets * np.log(predictions + 1e-9)) / N
    return ce


# average Manhattan Distance evaluation
def average_MD(targets, predictions):
    distances = []
    for target, prediction in zip(targets, predictions):
        # Compute the Manhattan Distance for a single pair
        distance = sum(abs(p - t) for p, t in zip(prediction, target))
        distances.append(round(distance, 5))
    # Compute and return the average Manhattan Distance
    average_distance = round(sum(distances) / len(distances), 5) if distances else 0
    return average_distance


# 4.1 Train the ensemble member
# Use the specified Transformer model (such as BERT) to train the input
# training data and save the trained model

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item


# Metrics function
def compute_metrics_with_extra_label(y_soft_label):
    def compute_metrics(eval_pred):
        probs, y_hard_label = eval_pred.predictions, eval_pred.label_ids
        # preds = np.argmax(probs, axis=1)
        # f1_micro_score
        f1_micro_score = f1_score(y_true=y_hard_label, y_pred=np.argmax(probs, axis=1), average='micro')
        # cross_entropy_score
        cross_entropy_score = sigmoid(cross_entropy(targets=y_soft_label, predictions=probs))
        # average_MD_score
        average_MD_score = float(average_MD(targets=y_soft_label, predictions=probs))
        return {"f1_micro": f1_micro_score, "cross_entropy": cross_entropy_score, "average_MD": average_MD_score}

    return compute_metrics


def train_transformer(X_train, y_train, X_dev, y_hard_dev, y_soft_dev, input_info,
                      model_name='bert-base-uncased', member_num=0):
    dataset = input_info['dataset']
    # split = input_info['split']
    run = input_info['run']
    if dataset == 'ConvAbuse':
        y_train = [1 if y < 0 else 0 for y in y_train]
    categories = list(set(y_hard_dev))
    print('y_hard_dev classes:', categories)

    # Tokenizer and model initialization
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(y_hard_dev)))

    # Tokenize the data
    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512, return_tensors="pt")
    train_labels = torch.tensor(y_train)
    dev_encodings = tokenizer(X_dev, truncation=True, padding=True, max_length=512, return_tensors="pt")
    dev_labels = torch.tensor(y_hard_dev)  # only train on hard labels for now

    # Dataset preparation
    train_dataset = CustomDataset(train_encodings, train_labels)
    dev_dataset = CustomDataset(dev_encodings, dev_labels)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=path + f"train_logs/{dataset}/checkpoints_{member_num}",
        per_device_train_batch_size=16,       # Adjust depending on your GPU memory
        per_device_eval_batch_size=32,        # Larger is okay for eval
        num_train_epochs=20,                   # Often enough for binary tasks
        learning_rate=2e-5, # hyperparam
        warmup_steps=500,
        weight_decay=0.01 * random.uniform(0.5, 1.5),  # Vary regularization
        logging_dir=path+f"train_logs/{dataset}/logs_{member_num}",
        logging_steps=50,                     # Log every 50 steps
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model=input_info['eval_metric'],
        # F1 score is designed to be maxinum, the other evaluation scores are designed to be minimized
        greater_is_better=True if input_info['eval_metric'] == 'f1_micro' else False,
        save_total_limit=1,
        report_to=[],
        # dataloader_pin_memory=False,  # Adds slight randomness
        # dataloader_drop_last=True  # Avoid perfect batching
    )

    # Trainer initialization
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=dev_dataset,
        compute_metrics=compute_metrics_with_extra_label(y_soft_dev)
    )

    # Training
    start = time.time()
    trainer.train()
    train_dur = time.time() - start

    if '/' in model_name:
        model_name = model_name.split('/')[1]

    # Save the model
    model_path = path + f"trained_models/BERT-hard-ce/{dataset}/{run}_{dataset}_{model_name}_{member_num}"
    Path(model_path).mkdir(parents=True, exist_ok=True)
    trainer.save_model(model_path)
    tokenizer_path = path + f"trained_models/Tokenizer/{model_name}"
    Path(tokenizer_path).mkdir(parents=True, exist_ok=True)
    tokenizer.save_pretrained(tokenizer_path)

    # Evaluation
    metrics = trainer.evaluate()
    metrics['train_dur'] = train_dur

    # Save model training statistics
    Path(path + f"train_logs/reports/").mkdir(parents=True, exist_ok=True)
    df = pd.DataFrame([metrics])
    df.to_csv(path + f'train_logs/reports/{run}_{dataset}_{model_name}_{member_num}.csv', index=False)
    df = pd.DataFrame(trainer.state.log_history)
    df.to_csv(path + f'train_logs/reports/log_{run}_{dataset}_{model_name}_{member_num}.csv', index=False)
    print(
        f"Finish training model {member_num} of {input_info['n_member']}, evaluated based on {input_info['eval_metric']}")
    del model
    del trainer


# 1. split data by dataset name
def split_data_by_dataset(input_path=path + 'Data/', output_path=path + 'Data/annotators/', split='train'):
    df = pd.read_csv(input_path + 'annotators.csv')
    for current_dataset in ['ArMIS', 'MD-Agreement', 'ConvAbuse', 'HS-Brexit']:
        new_df = df.loc[(df['dataset'] == current_dataset) & (df['split'] == split)]
        new_df.to_csv(output_path + f'{current_dataset}_{split}_candidates.csv', index=False)
    pass


# 2. get data according to dataset name
def get_data(dataset, split):
    df = pd.read_csv(path + f'Data/{dataset}_{split}.csv')
    X = df.text.tolist()
    y_hard = df.hard_label.tolist()
    y_soft = list(zip(df.soft_label_0.tolist(), df.soft_label_1.tolist()))
    return X, y_hard, y_soft


# 3. select weak candidates from all candidates, the data in candidates are from multiple annotators
def select_candidates(input_path=path + 'Data/annotators/',
                      current_dataset='MD-Agreement', split='train', candidates_choose_method='sample',
                      candidates_num=5):
    X_train_list = []
    y_train_list = []
    df = pd.read_csv(input_path + f'{current_dataset}_{split}_candidates.csv')
    # if candidates_choose_method == 'annotator', then select the most active annotators to train the model, otherwise select random samples
    if candidates_choose_method == 'annotator':
        annotators_counts = df['annotator'].value_counts()
        print("Available annotators and their annotated sample counts: ")
        print(annotators_counts)
        for i in range(min(candidates_num, len(annotators_counts))):
            selected_annotator = annotators_counts.index[i]
            candidates = df[df['annotator'] == selected_annotator]
            X_train_list.append(candidates.text.to_list())
            y_train_list.append(candidates.annotator_label.to_list())
    elif candidates_choose_method == 'all':
        print("All training data are used to train 1 model")
        X_train_list.append(df.text.to_list())
        y_train_list.append(df.annotator_label.to_list())
    else:
        print("Random select training data for ensemble models")
        grouped_candidates = df.groupby('id')
        for i in range(candidates_num):
            candidates = grouped_candidates.sample(n=1)
            X_train_list.append(candidates.text.to_list())
            # y_train_list.append(candidates.annotator_label.to_list())
            y_train_list.append(candidates.hard_label.to_list())
    return X_train_list, y_train_list


# 4. train n ensemble members (weak predictors)
def train_n_members(input_info, candidates_choose_method='annotator', start_num=0):
    # get train data for ensemble member training
    X_train_list, y_train_list = select_candidates(current_dataset=input_info['dataset'], split='train',
                                                   candidates_choose_method=candidates_choose_method,
                                                   candidates_num=input_info['n_member'])
    # get the dev data for trainer evaluation
    X_dev, y_hard_dev, y_soft_dev = get_data(input_info['dataset'], input_info['split'])
    if (len(X_train_list) < input_info['n_member']):
        print("!!! Only " + len(
            X_train_list) + " ensemble members can be trained due to insuffcient candidates in majority vote")
    for i in range(start_num, len(X_train_list)):
        print("Training ensemble members: run_", input_info['run'], " num_", i, " of ", input_info['n_member'])
        # train the ensemble member
        train_transformer(X_train_list[i], y_train_list[i], X_dev, y_hard_dev, y_soft_dev, input_info,
                          model_name=input_info['transformer_name'], member_num=i)

def main():
    for dataset in ['ArMIS', 'ConvAbuse', 'MD-Agreement', 'HS-Brexit']:
        model_name = 'aubmindlab/bert-base-arabertv2' if dataset == 'ArMIS' else 'bert-base-uncased'
        input_info = {
            # available dataset: ['ArMIS','ConvAbuse','MD-Agreement','HS-Brexit']
            'dataset': dataset,
            'n_member': 1,
            # ArMIS dataset contains Arabic, which is not supported by bert-base-uncased, so it should use bert-base-arabertv2 instead
            'transformer_name': model_name,
            'split': 'dev',
            # 'split':'test',
            # available eval_metric: ["f1_micro", "cross_entropy", "average_MD"]
            'eval_metric': 'cross_entropy',
            # for the initial test, alpha, beta, gamma should be all 1. the regularisation term mu is usually a value in [0.0001, 0.001,0.001, 0.1,1]
            'alpha': 1,
            'beta': 1,
            'gamma': 1,
            'mu': 1,
            'run': 0,
            'random_shuffle': 1
        }
        train_n_members(input_info, candidates_choose_method='sample')
        # X_test,_,_ = get_data(input_info["dataset"], input_info['split'])
        # member_hard_results,member_soft_results = get_results_from_all_member_models(X_test,input_info)
        # save_memeber_results(member_hard_results,member_soft_results,input_info["dataset"], input_info['split'])


if __name__ == '__main__':
    main()
    # dataset_name = 'ArMIS'
    # split = 'dev'
    # member_hard_results,member_soft_results = load_member_results(dataset_name,split)
    # print(member_hard_results,member_soft_results)
